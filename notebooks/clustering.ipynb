{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from scipy.linalg import sqrtm\n",
    "from scipy.spatial.distance import squareform\n",
    "from scipy.cluster.hierarchy import linkage, fcluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_folder = os.path.abspath('..')\n",
    "\n",
    "# === Load & parse data ===\n",
    "fitted_params = pd.read_csv(f'{project_folder}/results/gmm_results_1900thresh_diffscatter/gmm_fits_45g_1000s.csv')\n",
    "fitted_params['period'] = fitted_params['filename'].str.extract(r'_(\\d+)p_')[0].astype(int)\n",
    "fitted_params['scan'] = fitted_params['filename'].str.extract(r'p_(\\d+)\\.cbf')[0].astype(int)\n",
    "\n",
    "image_folder = os.path.join(project_folder, 'diffraction_images')\n",
    "files = sorted([f for f in os.listdir(image_folder) if f.endswith('.cbf')])\n",
    "roi_df = pd.read_csv(f'{project_folder}/results/ROI/ROI_800thresh.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reorder_clusters(group, labels):\n",
    "    \"\"\"Assign clusters based on lowest mean_y first, then mean_x.\"\"\"\n",
    "    cluster_means = pd.DataFrame({\n",
    "        'cluster': np.unique(labels),\n",
    "        'mean_x': [group.loc[labels == c, 'mean_x'].mean() for c in np.unique(labels)],\n",
    "        'mean_y': [group.loc[labels == c, 'mean_y'].mean() for c in np.unique(labels)]\n",
    "    })\n",
    "    \n",
    "    # Step 1: Find the cluster with the lowest mean_y (Cluster 1)\n",
    "    cluster_means = cluster_means.sort_values(by='mean_y')\n",
    "    cluster1 = cluster_means.iloc[0]  # First row is the lowest y\n",
    "    cluster1_label = cluster1['cluster']\n",
    "\n",
    "    # Step 2: Remove Cluster 1 and sort remaining clusters by mean_x\n",
    "    remaining_clusters = cluster_means[cluster_means['cluster'] != cluster1_label]\n",
    "    remaining_clusters = remaining_clusters.sort_values(by='mean_x', ascending=False).reset_index(drop=True)\n",
    "\n",
    "    # Step 3: Assign cluster indices\n",
    "    new_labels = {cluster1_label: 1}\n",
    "    for idx, row in enumerate(remaining_clusters.itertuples(), start=2):\n",
    "        new_labels[row.cluster] = idx\n",
    "\n",
    "    # Step 4: Apply the new cluster indices\n",
    "    return np.array([new_labels[label] for label in labels])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clustering data:: 100%|██████████| 16502/16502 [00:57<00:00, 287.50it/s]\n"
     ]
    }
   ],
   "source": [
    "# === Cluster assignment by scan ===\n",
    "for scan, group in tqdm(fitted_params.groupby('filename'), desc='Clustering data:'):\n",
    "    mu_x = group['mean_x'].values\n",
    "    mu_y = group['mean_y'].values\n",
    "    data = list(zip(mu_x, mu_y))\n",
    "\n",
    "    n_clusters = min(3, len(data))\n",
    "    kmeans = KMeans(n_clusters=n_clusters, init=\"k-means++\")\n",
    "    kmeans.fit(data)\n",
    "\n",
    "    labels = kmeans.predict(data)\n",
    "    sorted_labels = reorder_clusters(group, labels)\n",
    "    fitted_params.loc[group.index, 'cluster'] = sorted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_params.to_csv(f'{project_folder}/results/cluster_results/kmeans_plusinit.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kmeans Wasserstein Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sqrtm_psd(mat):\n",
    "    \"\"\"Compute the square root of a 2x2 positive semi-definite matrix.\"\"\"\n",
    "    eigvals, eigvecs = np.linalg.eigh(mat)\n",
    "    sqrt_eigvals = np.sqrt(np.maximum(eigvals, 0))\n",
    "    return eigvecs @ np.diag(sqrt_eigvals) @ eigvecs.T\n",
    "\n",
    "def gaussian_w2(mu1, cov1, mu2, cov2):\n",
    "    \"\"\"2-Wasserstein distance between two 2D Gaussians.\"\"\"\n",
    "    mean_term = np.sum((mu1 - mu2) ** 2)\n",
    "    sqrt_cov2 = sqrtm_psd(cov2)\n",
    "    cov_product = sqrtm_psd(sqrt_cov2 @ cov1 @ sqrt_cov2)\n",
    "    cov_term = np.trace(cov1 + cov2 - 2 * cov_product)\n",
    "    return np.sqrt(mean_term + cov_term)\n",
    "\n",
    "def pairwise_gaussian_w2(means, covs):\n",
    "    \"\"\"Compute all pairwise 2-Wasserstein distances between Gaussians.\"\"\"\n",
    "    n = len(means)\n",
    "    D = np.zeros((n, n))\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            d = gaussian_w2(means[i], covs[i], means[j], covs[j])\n",
    "            D[i, j] = D[j, i] = d\n",
    "    return D\n",
    "\n",
    "def build_cov_matrix(sx, sy, rho):\n",
    "    \"\"\"Convert std devs and correlation into covariance matrix.\"\"\"\n",
    "    return np.array([\n",
    "        [sx**2, rho * sx * sy],\n",
    "        [rho * sx * sy, sy**2]\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clustering Gaussians: 100%|██████████| 16502/16502 [12:03<00:00, 22.82it/s]\n"
     ]
    }
   ],
   "source": [
    "# === Clustering by scan ===\n",
    "for scan, group in tqdm(fitted_params.groupby('filename'), desc='Clustering Gaussians'):\n",
    "    mu = group[['mean_x', 'mean_y']].values\n",
    "    sigmas = group[['sigma_x', 'sigma_y']].values\n",
    "    rhos = group['rho'].values\n",
    "\n",
    "    covs = [build_cov_matrix(sx, sy, rho) for (sx, sy), rho in zip(sigmas, rhos)]\n",
    "    \n",
    "    D = pairwise_gaussian_w2(mu, covs)\n",
    "\n",
    "    n_clusters = min(3, len(mu))\n",
    "    kmedoids = KMedoids(n_clusters=n_clusters, metric='precomputed', init='k-medoids++', random_state=0)\n",
    "    kmedoids.fit(D)\n",
    "\n",
    "    labels = kmedoids.labels_\n",
    "    sorted_labels = reorder_clusters(group, labels)  # optional: normalize label order\n",
    "    fitted_params.loc[group.index, 'cluster'] = sorted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_params.to_csv(f'{project_folder}/results/cluster_results/kmedoid_wasserstein.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gaussian_params(row):\n",
    "    mu = np.array([row['mean_x'], row['mean_y']])\n",
    "    sigma_x = row['sigma_x']\n",
    "    sigma_y = row['sigma_y']\n",
    "    rho = row['rho']\n",
    "    cov = np.array([[sigma_x**2, rho * sigma_x * sigma_y],\n",
    "                    [rho * sigma_x * sigma_y, sigma_y**2]])\n",
    "    return mu, cov\n",
    "\n",
    "def wasserstein_2(mu1, cov1, mu2, cov2):\n",
    "    diff = mu1 - mu2\n",
    "    sqrt_cov1 = sqrtm(cov1)\n",
    "    inner = sqrtm(sqrt_cov1 @ cov2 @ sqrt_cov1)\n",
    "    if np.iscomplexobj(inner):\n",
    "        inner = inner.real  # drop imaginary noise\n",
    "    return np.linalg.norm(diff)**2 + np.trace(cov1 + cov2 - 2 * inner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clustering data:: 100%|██████████| 16502/16502 [1:19:38<00:00,  3.45it/s]\n"
     ]
    }
   ],
   "source": [
    "for filename, group in tqdm(fitted_params.groupby('filename'), desc='Clustering data:'):\n",
    "    # Filter the row corresponding to the current filename\n",
    "    roi_row = roi_df[roi_df['filename'] == filename]\n",
    "\n",
    "    params_df = fitted_params[fitted_params['filename']==filename].copy()\n",
    "\n",
    "    n = len(params_df)\n",
    "    dist_matrix = np.zeros((n, n))\n",
    "\n",
    "    gaussians = [get_gaussian_params(row) for _, row in params_df.iterrows()]\n",
    "\n",
    "    for i in range(n):\n",
    "        for j in range(i + 1, n):\n",
    "            mu1, cov1 = gaussians[i]\n",
    "            mu2, cov2 = gaussians[j]\n",
    "            dist = wasserstein_2(mu1, cov1, mu2, cov2)\n",
    "            dist_matrix[i, j] = dist_matrix[j, i] = dist\n",
    "\n",
    "    # Convert to condensed form\n",
    "    condensed_dist = squareform(dist_matrix)\n",
    "\n",
    "    # Linkage (use 'average', 'complete', 'ward' etc.)\n",
    "    Z = linkage(condensed_dist, method='ward')\n",
    "\n",
    "    # Get flat clusters (e.g., force 3 clusters)\n",
    "    cluster_labels = fcluster(Z, t=3, criterion='maxclust')\n",
    "    sorted_labels = reorder_clusters(group, cluster_labels)\n",
    "    fitted_params.loc[group.index, 'cluster'] = sorted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_params.to_csv(f'{project_folder}/results/cluster_results/hierarchical_wasserstein_wardlinkage.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Global Params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_combined_gaussian_params(df):\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    A = df['amplitude'].to_numpy()\n",
    "    weights = A / np.sum(A)\n",
    "    mu_x = np.sum(weights * df['mean_x'].to_numpy())\n",
    "    mu_y = np.sum(weights * df['mean_y'].to_numpy())\n",
    "\n",
    "    total_intensity = 0\n",
    "    Sigma = np.zeros((2, 2))\n",
    "\n",
    "    for i, row in df.iterrows():\n",
    "        sigma_x_i, sigma_y_i, rho_i = row['sigma_x'], row['sigma_y'], row['rho']\n",
    "        mu_x_i, mu_y_i = row['mean_x'], row['mean_y']\n",
    "        Sigma_i = np.array([\n",
    "            [sigma_x_i**2, rho_i * sigma_x_i * sigma_y_i],\n",
    "            [rho_i * sigma_x_i * sigma_y_i, sigma_y_i**2]\n",
    "        ])\n",
    "\n",
    "        det_Sigma_i = np.linalg.det(Sigma_i)\n",
    "        intensity_i = row['amplitude'] * np.sqrt((2 * np.pi)**2 * det_Sigma_i)\n",
    "        \n",
    "        # Add to the total intensity\n",
    "        total_intensity += intensity_i\n",
    "\n",
    "        mean_diff = np.array([[mu_x_i - mu_x], [mu_y_i - mu_y]])\n",
    "        Sigma += weights[i] * (Sigma_i + mean_diff @ mean_diff.T)\n",
    "\n",
    "    sigma_x_total = np.sqrt(Sigma[0, 0])\n",
    "    sigma_y_total = np.sqrt(Sigma[1, 1])\n",
    "    rho_total = Sigma[0, 1] / (sigma_x_total * sigma_y_total)\n",
    "\n",
    "    return total_intensity, mu_x, mu_y, sigma_x_total, sigma_y_total, rho_total\n",
    "\n",
    "\n",
    "def compute_gaussian_rotation(sigma_x, sigma_y, rho):\n",
    "    # Covariance matrixlinal\n",
    "    Sigma = np.array([[sigma_x**2, rho * sigma_x * sigma_y], \n",
    "                      [rho * sigma_x * sigma_y, sigma_y**2]])\n",
    "\n",
    "    # Eigen decomposition\n",
    "    eigenvalues, eigenvectors = np.linalg.eigh(Sigma)\n",
    "\n",
    "    # Sort eigenvalues and eigenvectors (largest eigenvalue first)\n",
    "    idx = np.argsort(eigenvalues)[::-1]\n",
    "    eigenvalues = eigenvalues[idx]\n",
    "    eigenvectors = eigenvectors[:, idx]\n",
    "\n",
    "    # Compute sigma_major and sigma_minor\n",
    "    sigma_major = np.sqrt(eigenvalues[0])\n",
    "    sigma_minor = np.sqrt(eigenvalues[1])\n",
    "\n",
    "    # Compute rotation angle phi\n",
    "    phi = np.arctan2(eigenvectors[1, 0], eigenvectors[0, 0])\n",
    "\n",
    "    return sigma_major, sigma_minor, phi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fitted_params=pd.read_csv('final_results_clustering/kmeans_plusinit.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16502/16502 [02:15<00:00, 122.16it/s]\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for _, group in tqdm(fitted_params.groupby('filename')):\n",
    "    for cluster in group['cluster'].unique():\n",
    "        cluster_group = group[group['cluster'] == cluster]\n",
    "\n",
    "        if cluster_group.empty:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            intensity, mu_x, mu_y, sigma_x, sigma_y, rho = compute_combined_gaussian_params(cluster_group)\n",
    "            sigma_major, sigma_minor, phi = compute_gaussian_rotation(sigma_x, sigma_y, rho)\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping cluster {cluster} in {group['filename'].iloc[0]} due to error: {e}\")\n",
    "            continue\n",
    "\n",
    "        results.append({\n",
    "            'filename': cluster_group['filename'].iloc[0],\n",
    "            'cluster': cluster,\n",
    "            'intensity': intensity,\n",
    "            'mean_x': mu_x,\n",
    "            'mean_y': mu_y,\n",
    "            'sigma_x': sigma_x,\n",
    "            'sigma_y': sigma_y,\n",
    "            'rho': rho,\n",
    "            'fwhm_major': sigma_major * 2.3548,\n",
    "            'fwhm_minor': sigma_minor * 2.3548,\n",
    "            'phi': phi,\n",
    "        })\n",
    "\n",
    "all_results_df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results_df.to_csv(f'{project_folder}/results/cluster_results/spot_features/hierarchical_wasserstein_wardlinkage_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
